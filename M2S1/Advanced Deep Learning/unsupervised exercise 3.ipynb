{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46d662",
   "metadata": {
    "id": "7a46d662"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, InputSpec\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-50DOxG9Yfxh",
   "metadata": {
    "id": "-50DOxG9Yfxh"
   },
   "source": [
    "# Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KIx00DQfYerp",
   "metadata": {
    "id": "KIx00DQfYerp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow import keras\n",
    "\n",
    "# Function to plot PCA and t-SNE embeddings side-by-side\n",
    "def plotfun(pca_embedding, tsne_embedding, y_data, labels, title, fname=None):\n",
    "    if fname is not None:\n",
    "        # If saving plot do not display it so that execution is not blocked\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')\n",
    "    from matplotlib import pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    for i in labels:\n",
    "        ax.scatter(pca_embedding[y_data == i, 0], pca_embedding[y_data == i, 1], pca_embedding[y_data == i, 2], label=i)\n",
    "    ax.legend()\n",
    "    ax.set_title(title + ' PCA Embedding')\n",
    "    ax.set_xlabel('PCA 1')\n",
    "    ax.set_ylabel('PCA 2')\n",
    "    ax.set_zlabel('PCA 3')\n",
    "    \n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    for i in labels:\n",
    "        ax.scatter(tsne_embedding[y_data == i, 0], tsne_embedding[y_data == i, 1], tsne_embedding[y_data == i, 2], label=i)\n",
    "    ax.legend()\n",
    "    ax.set_title(title + ' t-SNE Embedding')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    ax.set_zlabel('t-SNE 3')\n",
    "\n",
    "    if fname is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(fname)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Function to get and prepare the MNIST dataset\n",
    "def get_MNIST_data(number_of_test_samples):\n",
    "    ############\n",
    "    # Prepare the data\n",
    "    ############\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    mnist_dataset = keras.datasets.mnist.load_data()\n",
    "    (trainset, testset) = (mnist_dataset[0], mnist_dataset[1])\n",
    "    (X_train, y_train) = trainset\n",
    "    (X_test, y_test) = testset\n",
    "\n",
    "    # Preprocess data (convert to float and scale to between 0 and 1)\n",
    "    X_train = X_train.astype('float32') / 255\n",
    "    X_test = X_test.astype('float32') / 255\n",
    "\n",
    "    # Flatten data (turn images into vectors)\n",
    "    X_train_fl = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_fl = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Take a random subset of the data\n",
    "    rndperm = np.random.permutation(X_test.shape[0])\n",
    "    if number_of_test_samples > 0:\n",
    "        X_test = X_test[rndperm[:number_of_test_samples],]\n",
    "        X_test_fl = X_test_fl[rndperm[:number_of_test_samples],]\n",
    "        y_test = y_test[rndperm[:number_of_test_samples],]\n",
    "\n",
    "    target_ids = np.unique(y_train)\n",
    "\n",
    "    return X_train, X_train_fl, y_train, X_test, X_test_fl, y_test, target_ids\n",
    "\n",
    "\n",
    "# Function to embed data using t-SNE and PCA\n",
    "def get_tsne_pca(data_fl, n_components=2, tsne_perplexity=30):\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_embedding = pca.fit_transform(data_fl)\n",
    "    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=n_components, perplexity=tsne_perplexity)\n",
    "    tsne_embedding = tsne.fit_transform(data_fl)\n",
    "    print('t-SNE embedding KL Divergence: {}'.format(tsne.kl_divergence_))\n",
    "\n",
    "    return pca_embedding, tsne_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R2Pkr1_jZ8Gc",
   "metadata": {
    "id": "R2Pkr1_jZ8Gc"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y-E-clHdZ7XM",
   "metadata": {
    "id": "Y-E-clHdZ7XM"
   },
   "outputs": [],
   "source": [
    "calcnmi = normalized_mutual_info_score\n",
    "calcari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def calcacc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in zip(row_ind, col_ind)]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-ElnkQoJYNCb",
   "metadata": {
    "id": "-ElnkQoJYNCb"
   },
   "source": [
    "# DEC Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gOSO9r1-YLtY",
   "metadata": {
    "id": "gOSO9r1-YLtY"
   },
   "outputs": [],
   "source": [
    "# Assuming calcacc, calcnmi, calcari functions are defined elsewhere\n",
    "# These are utility functions to compute accuracy, NMI, and ARI\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```python\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` which represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self, autoencoder, n_clusters=10, alpha=1.0):\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if int(epochs/10) != 0 and epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer('encoded').output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (calcacc(self.y, y_pred), calcnmi(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time.time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: %ds' % round(time.time() - t0))\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extract features from the encoder part of the autoencoder.\"\"\"\n",
    "        feature_model = Model(self.model.input, self.encoder.output)\n",
    "        return feature_model.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, test_data=None, save_dir='./DEC_results/'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time.time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)  # <-- Completed\n",
    "        y_pred = kmeans.fit_predict(self.extract_features(x))  # <-- Completed\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(calcacc(y, y_pred), 5)\n",
    "                    nmi = np.round(calcnmi(y, y_pred), 5)\n",
    "                    ari = np.round(calcari(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # Reconstruction\n",
    "                decoded_images = self.autoencoder.predict(x)  # <-- Completed\n",
    "                mse = np.mean(np.square(test_data[0] - decoded_images))\n",
    "                print('Iter %d: test reconstruction mse = %.5f' % (ite, mse))\n",
    "\n",
    "                n_images = 5\n",
    "                import matplotlib\n",
    "                matplotlib.use('Agg')\n",
    "                from matplotlib import pyplot as plt\n",
    "                for i in range(n_images):\n",
    "                    fig = plt.figure()\n",
    "                    ax = fig.add_subplot(121)\n",
    "                    ax.imshow(test_data[0][i].reshape(28, 28), cmap='gray')\n",
    "                    ax.set_title('Test Image ' + str(i + 1))\n",
    "                    ax = fig.add_subplot(122)\n",
    "                    ax.imshow(decoded_images[i].reshape(28, 28), cmap='gray')\n",
    "                    ax.set_title('Reconstructed Image ' + str(i + 1))\n",
    "                    plt.savefig(f\"{save_dir}/reconstructed_image_{i+1}_iter_{ite}.png\")\n",
    "                    plt.close(fig)\n",
    "\n",
    "            # mini-batch k-means\n",
    "            idx = index_array[index: min(index + batch_size, x.shape[0])]\n",
    "            q = self.model.predict(x[idx], verbose=0)\n",
    "            p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "            loss = self.model.train_on_batch(x[idx], p)\n",
    "\n",
    "            index += batch_size\n",
    "            if index >= x.shape[0]:\n",
    "                index = 0\n",
    "\n",
    "        logfile.close()\n",
    "        print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lj7390GzeAQ4",
   "metadata": {
    "id": "lj7390GzeAQ4"
   },
   "source": [
    "# DEC Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Du9aHRfKd_6u",
   "metadata": {
    "id": "Du9aHRfKd_6u"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def create_autoencoder(input_size, hidden_units, act='relu'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        input_size: number of units in the input layer\n",
    "        hidden_units: list of number of units in each layer of encoder. hidden_units[-1] is size (number of units) of the encoded representation.\n",
    "            The decoder is symmetric with encoder.\n",
    "        act: activation, only applied to hidden layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    output_size = input_size\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_size,))\n",
    "\n",
    "    # Encoder\n",
    "    x = input_layer\n",
    "    for units in hidden_units:\n",
    "        x = Dense(units, activation=act)(x)\n",
    "    encoded = Dense(hidden_units[-1], activation='linear', name='encoded')(x)  # Encoded representation\n",
    "\n",
    "    # Decoder (symmetric to encoder)\n",
    "    x = encoded\n",
    "    for units in reversed(hidden_units[:-1]):  # Skip the last layer since it's the encoded representation\n",
    "        x = Dense(units, activation=act)(x)\n",
    "    decoded = Dense(output_size, activation='sigmoid')(x)  # Output layer\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=decoded, name='AE'), Model(inputs=input_layer, outputs=encoded, name='encoder')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IMMH3ozMdiL7",
   "metadata": {
    "id": "IMMH3ozMdiL7"
   },
   "source": [
    "# DEC Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvfHUBhMXbNb",
   "metadata": {
    "id": "wvfHUBhMXbNb"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    number_of_test_samples = 500  # Limiting to 500 test samples\n",
    "    tsne_perplexity = 25\n",
    "\n",
    "    X_train, X_train_fl, y_train, X_test, X_test_fl, y_test, target_ids = get_MNIST_data(number_of_test_samples)\n",
    "\n",
    "    #########\n",
    "    # Deep Clustering\n",
    "    #########\n",
    "\n",
    "    n_clusters = len(target_ids)\n",
    "    input_size = 28 * 28\n",
    "    hidden_units = [128, 64, 32]\n",
    "    epochs = 20\n",
    "    batch_size = 256\n",
    "\n",
    "    save_dir = './DEC_results/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    autoencoder = create_autoencoder(input_size, hidden_units)\n",
    "    optimiser = Adadelta(learning_rate=10.0)\n",
    "    dec = DEC(autoencoder, n_clusters=n_clusters)\n",
    "\n",
    "    # Pretrain the autoencoder\n",
    "    dec.pretrain(x=X_train_fl, y=y_train,\n",
    "                 optimizer=optimiser,\n",
    "                 epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 save_dir=save_dir)\n",
    "\n",
    "    dec.model.summary()\n",
    "    dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "\n",
    "    update_interval = 100\n",
    "\n",
    "    # Ensure that the test_data contains the same number of samples\n",
    "    y_pred = dec.fit(X_train_fl, y=y_train, \n",
    "                     maxiter=3e3, \n",
    "                     update_interval=update_interval, \n",
    "                     save_dir=save_dir, \n",
    "                     test_data=(X_test_fl[:number_of_test_samples], y_test[:number_of_test_samples], target_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d6d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38008e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
